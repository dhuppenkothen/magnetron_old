% This document is part of the transientdict project.
% Copyright 2013 the authors.

\documentclass[12pt]{emulateapj}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{epsfig}
\usepackage{times}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{bm}
\usepackage{hyperref}
%\usepackage[stable]{footmisc}
%\usepackage{color}
%\bibliographystyle{apj}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Fermi}{\project{Fermi}}
\newcommand{\RXTE}{\project{RXTE}}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\counts}{y}
\newcommand{\pars}{\theta}
\newcommand{\mean}{\lambda}
\newcommand{\likelihood}{{\mathcal L}}
\newcommand{\Poisson}{{\mathcal P}}
\newcommand{\Uniform}{{\mathcal U}}
\newcommand{\bg}{\mathrm{bg}}
\newcommand{\word}{\phi}
%\newcommand{\bs}{\boldsymbol}

\begin{document}

\title{Dissecting Magnetar Variability with Probabilistic Generative Models}

\author{D. Huppenkothen\altaffilmark{1, 2}, B. Brewer\altaffilmark{}, D. Hogg\altaffilmark{}, I. Murray\altaffilmark{}, M. Frean\altaffilmark{}, C. Elenbaas\altaffilmark{1}, A. L. Watts\altaffilmark{1}, Y.Levin\altaffilmark{},  L. Heil\altaffilmark{1}, C. Kouveliotou\altaffilmark{3,4}}

 %%% NEED TO ADD ALEXANDER VAN DER HORST?
 
\altaffiltext{1}{Anton Pannekoek Institute for Astronomy, University of
  Amsterdam, Postbus 94249, 1090 GE Amsterdam, the Netherlands}
  \altaffiltext{2}{Email: D.Huppenkothen@uva.nl}
\altaffiltext{3}{Astrophysics Office, ZP 12, NASA-Marshall Space Flight Center, Huntsville, AL 35812, USA}
\altaffiltext{4}{NSSTC, 320 Sparkman Drive, Huntsville, AL 35805, USA}

\altaffiltext{}{Monash Center for Astrophysics and School of Physics, Monash University, Clayton, Victoria 3800, Australia}


\begin{abstract}
%Magnetars are whatever and people need whatever.
%Magnetars produce bursts,
%  each of which appears to be composed of a set of one or more spikes.
%Here we build a probabilistic generative model for the \Fermi\ GBM photon data on Magnetar bursts.
%The spikes appear to be somewhat asymmetric
%  (rise looks different from decay),
%  have various amplitudes,
%  but have widths that appear to be similar \emph{within} each burst.
%We model each burst as being composed of a mixture of spikes,
%  where each spike is a scaled, stretched, and shifted version of a universal dimensionless function.
%We perform probabilistic inference with a Poisson likelihood and vague priors to determine
%  the positions, amplitudes, and positions of the spikes,
%  the width of the spikes in each burst,
%  and the shape of the dimensionless spike function.
%The phenomenology of spike multiplicity, shape, and width is discussed.
\end{abstract}

\keywords{pulsars: individual (SGR J1550-5418), stars: magnetic fields, stars: neutron, X-rays: bursts, methods:statistics}

\section{Introduction}

With current and upcoming telescopes monitoring the sky regularly across the entire electromagnetic spectrum, time domain astronomy is emerging as one of the key fields in which major 
new discoveries are being made.  A large fraction of astrophysical sources are known to be variable. The timescales span more than five orders of magnitude: fast oscillations in 
X-ray binaries (XRBs) change over milliseconds \citep[e.g.][]{xrb_khzqpos}, while red giants are believed to change over decades or even centuries \citep[e.g.][]{dasch_giants}. 
Variability studies have the potential to unravel fundamental physical processes: studying variability in XRBs can help us unravel accretion processes and constrain theories of viscosity. 
Similarly, giant flares from magnetars have the potential to map the neutron star interior via neutron star seismology. 

While much of the variability exhibited for example in XRBs is accessible by standard Fourier methodology, this is not true for an important group of sources: for any transients 
with complex temporal structure where the relevant time scales of interest are of the same order as the typical time scales in the system,  this variability introduces considerable 
systematic errors into inferences made from this type of data with standard methods. Three examples stand out particularly: solar flares, $\gamma$-ray bursts (GRBs) and magnetar bursts. 
All three phenomena are characterised by emission at high (X-ray to soft $\gamma$-ray) energies, bursts lasting from $\sim 1/10$ of a second to hundreds of seconds, and a 
complex temporal structure that varies strongly from burst to burst (for an example, see Figure \ref{fig:example_bursts}). Fourier methods are hampered by the lack of knowledge 
of the overall structure of the burst, and consequently the lack of knowledge of both the shape and the statistical distribution of powers in the periodogram at low frequencies. Conversely, 
it is difficult to postulate a common model applicable to a large sample of light curves of these sources. Any light curve model must be flexible enough to account for differences between bursts. 
At the same time, there is a distinct lack of understanding of the underlying physical processes leading to variability in these bursts to inform a choice of model. Here, we aim to develop a probabilistic 
model for highly variable transient events, based on a decomposition of the light curve into simple shapes, without knowing the number of components in the model a priori. We demonstrate the 
power of this approach on a large sample of magnetar bursts, and, for the first time, connect variability in magnetar bursts to the time scales thought to govern the underlying physics.
\begin{figure*}[h]
\begin{center}
\includegraphics[width=18cm]{example_bursts.pdf}
\caption{}
\label{fig:example_bursts}
\end{center}
\end{figure*}


Neutron stars, the ultra-dense compact remnants of core-collapse supernovae, are the prime laboratory for studying nuclear 
physical processes in a parameter regime of density and pressure inaccessible to experiments on Earth. 
Among the veritable zoo of neutron star phenomena, two stand out for their peculiar parameters: Soft Gamma Repeaters (SGRs),
named after their hallmark recurring bursts in hard X-rays, and Anomalous X-ray Pulsars (AXPs), persistent pulsating X-ray
sources with a conspicuous paucity of radio counterparts \citep[although at this point, three sources out of a total of $26$ candidates 
have confirmed radio detections: ][]{halpern2005,camilo2006,malofeev2010}. 

While SGRs and AXPs were detected as separate phenomena, they share common properties \citep[for general overviews of both phenomena, see ][]{woods2006,mereghetti2011}: slow spin periods between
$2 - 12 \, \mathrm{s}$, coupled with generally high period derivatives, lead to large inferred dipole magnetic fields of
the order of $10^{14} \, \mathrm{G}$, well above the quantum-critical limit $B_{\mathrm{QED}} = 4.4 \times 10^{13} \, \mathrm{G}$,
where quantum effects such as pair production and photon splitting become important \citep[although three sources have been 
identified with properties similar to SGRs and AXPs, but inferred dipole fields below this limit;[]{vanderhorst2010,esposito2010,rea2010,rea2012,scholz2012,rea2014}. 

Both AXPs and SGRs are believed to be the observable phenomena of the same underlying physical system: a highly magnetised
neutron star, also called a magnetar \citep{duncan1992,thompson1995}. In the original scenario, the observable X-ray emission, both persistent emission and bursts, 
is powered by the slow evolution and decay of the source's strong magnetic field, instead of the loss of rotational energy as 
is generally the case for standard radio pulsars \citep{thompson1995,thompson2001}. 

SGR bursts are of particular interest for a variety of reasons. Most importantly, the observation of three giant flares, catastrophic
outbursts in hard X-rays of energies up to $10^{47} \, \mathrm{erg}$\citep[e.g.][]{mazets1979,cline1998,hurley1999,feroci1999,palmer2005,hurley2004,hurley2005, borkowski2004, mereghetti2005,mazets2005,yamazaki2005,terasawa2005}, and the associated detection of quasi-periodic oscillations (QPOs)
in the tails of these flares \citep{israel2005,strohmayer2005,strohmayer2006,watts2006}, have opened up a potential new way to look into the neutron star interior via asteroseismology. In the original 
model, these giant flares are caused by a catastrophic re-ordering of the neutron star's internal field. As the magnetic field lines, frozen into
the solid crust, move, the crust responds by fracturing when its breaking strain is exceeded by the forces exerted by the magnetic field.
The energy released in this process is both released into the magnetosphere, where it forms a pair plasma that slowly radiates away and
produces the observed emission, as well as seismic waves throughout the star. The frequencies of the oscillatory modes depend on
the nuclear physics of the star's crust and core, making them a prime diagnostic for the conditions inside the star.

The crust quake model of \citet{thompson1995} is not the only viable model
for the production of magnetar giant flares. \citet{thompson1995} postulated a catastrophic untwisting the internal magnetic field, which leads to mechanical failure of the neutron star crust.
However, it is possible for the internal magnetic field to unwind slowly, allowing the external magnetic field to follow suit without mechanical failure of the crust. 
A catastrophic energy release could then occur in the magnetosphere itself, facilitated by explosive reconnection \citep{lyutikov2003,lyutikov2006,heyl2005,nakar2005,masada2010,gill2010}. 
\citet{vanhoven2011} have shown that a giant flare 
triggered in the magnetosphere could still excite large-scale shear motions in the crust. In this case, the giant flare would precede the crust quake, and not the other way around.
A second argument for this alternative picture, akin more to solar flares than earthquakes, comes from calculations that indicate that the physical conditions in the crust
do not allow for mechanical (brittle) failure \citep{jones2003,horowitz2009}. Instead, the crust is more likely to admit ductile failure and plastic deformation \citep{levin2012}.
[WHAT DOES THIS MEAN FOR TIME SCALES IN BURSTS?]

While giant flares are relatively rare phenomena (we have only observed $3$ in the last $35$ years), magnetars also show a complex
behaviour of emitting much smaller, shorter recurrent bursts. These bursts are of the order of $<1\mathrm{s}$ long, with energies
generally between $10^{38}\,\mathrm{erg}$ and $10^{41}\,\mathrm{erg}$, and have a complex temporal structure with single or
multiple peaks that differs from one burst to the next. They are observed either appearing individually, or in burst 
storms, where tens or hundreds of bursts can occur over a timescale of single days to weeks\citep{mazets1999,goetz2006b,israel2008,mereghetti2009,savchenko2010,israel2010,scholz2011,dib2012,vanderhorst2012,vonkienlin2012}. 
The appearance of bursts appears to be random\citep{gogus1999,gogus2000}, but far more numerous than the giant flares: 
for the two best-observed magnetars, SGR 1806-20 and SGR 1900+14, the
data set spans thousands of such bursts. 

One observation of particular interest concerns the overall distributions of these bursts: the differential distribution of fluence (integrated flux) and 
the cumulative distribution of waiting times are similar to those observed in earth quakes and solar flares\citep{cheng1996}. Especially the fluence distribution
can be well-modelled by a power law with a power-law index of $\sim 1.7$, believed to be a typical signature for a system obeying 
self-organised criticality (SOC; \citealp{bak1987,bak1988}; for a recent introduction and review on SOC in astrophysics see \citealp{aschwanden2014}.). 
In the SOC framework, the physical system in question continuously drives itself towards a critical state,
without requiring fine tuning. When the critical state is reached, relaxation occurs via a catastrophic release of energy. The system returns
to a subcritical state, and the cycle begins anew. The standard example of an SOC system is a sand pile: as sand grains are slowly dropped onto
the pile, the slope of the sand pile steepens gradually. However, there is a critical slope, at which the sand pile can no longer support the 
additional grains dropped onto it, and relaxation occurs via an avalanche, returning the slope to a sub-critical state. 
One advantage of describing a system in the SOC framework is that while the details of the process leading to the critical state and subsequent
relaxation depend on the physical processes specific to the system, many of the overall statistical properties are universal.

At present, it is not clear whether magnetar bursts are smaller-scale versions of the crust fracture process believed to produce a giant flare,
or perhaps entirely magnetospheric events. In the latter scenario, the evolution of the magnetic field would operate on timescales
slow enough for the crust to respond with plastic deformation rather than brittle fracture, and the energy release would proceed via
explosive reconnection in the magnetosphere instead.

%%% do these models predict different timescales, and can we say something here about distinguishing them?

Because of the bursts' complex temporal morphology, differing from burst to burst, it is difficult to extract information from their temporal evolution. 
Many bursts show a pattern of one or multiple spikes, where these spikes sometimes sit on top of each other. To extract parameters that could be related
to physical quantities such as the rise time of each spike and the waiting time distribution between spikes, it is necessary to 
extract both from the data in an unbiased way. At the same time, because of the much larger data set SGR bursts offer, this process
needs to operate automatically without the user's intervention.

In this paper, we propose a new method to model magnetar bursts as a linear combination of simple shapes. Both the number of components
per burst as well as the model parameters for each components are free parameters, to be explored using Diffusive Nested Sampling.
We apply this model to a large data set of bursts from SGR J1550-5418, observed with the Gamma-ray Burst Monitor (GBM) on board the 
\Fermi spacecraft. SGR J1550-5418 (also 1E 1547.0-5408) was first observed with the {\it Einstein} X-ray observatory \citep{lamb1981}
 and subsequently classified as an AXP based on its soft X-ray spectrum and possible association with a supernova remnant \citep{gelfand2007}.
A radio detection of pulsations with a slow spin period of $P = 2.096\mathrm{s}$ and a spin-down of $\dot{P} = 2.318 \times 10^{-14}$, implying a magnetic field of $3.2 \times 10^{14} \, \mathrm{G}$ \citep{camilo2007}, confirmed its classification as a magnetar.

In 2008 and 2009, SGR J1550-5418 exhibited three major bursting episodes (October 2008, January 2009 and March/April 2009), where the 2009 January episode is of special interest as a
burst storm, where hundreds of bursts were observed within a single day with various X-ray telescopes ({\it Swift} Burst Alert Telescope (BAT), \citealp{israel2010, scholz2011}); the \Fermi Gamma-Ray Burst Monitor (GBM), \citealp{kaneko2010,vonkienlin2012,vanderhorst2012}; the {\it Rossi X-ray Timing Explorer} (RXTE), \citep{dib2012}, and two main instruments on board the {\it INTEGRAL} spacecraft \citealp{mereghetti2009, savchenko2010})
 
 Here, we use a sample of bursts observed with {\it Fermi}/GBM during all three bursting episodes. The high sensitivity of the instrument as well as 
 the large number of observed bursts make this source an excellent target to demonstrate the power of the proposed methods. For the first time, 
 we extract a wealth of physically relevant time scales from SGR bursts. 
 We place these timescales in the context of the SOC framework, and tie them to physical parameters in the system.


%Magnetars make bursts.
%Magnetar bursts have many spikes each.
%The spikes look similar within each burst.
%That motivates a very simple model,
%  in which each burst is made up of a sum of spike models, which we call words.

\section{Data}

Bursts from SGR J1550-5418 triggered \Fermi/GBM for a total of $126$ times between 2008 October 3 and 2009 April 17, with $\sim 450$ bursts observed on its most active day, 2009 January 22, alone. 
Each trigger records data from $30\,\mathrm{s}$ before each trigger to $300\,\mathrm{s}$ after each trigger, upon which the instrument cannot trigger for another $\sim 300 \,\mathrm{s}$. 
After GBM is triggered, subsequent bursts within this time span do not trigger the instrument, but can be found in an untriggered burst search. The resulting data stream consists of 
the arrival times of individual photons (time-tagged events, TTE), with an intrinsic time resolution of $\sim 2\mu\mathrm{s}$, sufficient for probing variability to very short time scales.
We use data from the $12$ NaI detectors, whose energy range of $8$ keV to $4$ MeV is sufficient, since SGR bursts rarely exhibit radiation above $200$ keV. Additionally, we only used detectors with viewing angles to the source $< 60^{\circ}$, and checked whether the source was occulted by the spacecraft and the other instrument, the Large Area Detector (LAT). We use the combined samples of bursts from \citet{vonkienlin2012} and \citet{vanderhorst2012}. 
We extracted TTE data between $8 \, \mathrm{keV}$ and $200 \, \mathrm{keV}$ around each burst, starting at $t_{\mathrm{start}} - 0.1 \times\mathrm{T}90$ (the burst duration, $\mathrm{T}90$, is defined as the time in which the central $90\%$ of the photons, starting at $5\%$ and ending at $95\%$, reach the detector) and ending at $t_{\mathrm{start}} + 1.1\times\mathrm{T}90$ in order to ensure the entire burst is within our data set. Photon arrival times are barycentered, i.e. projected to the centre of mass of the solar system, to account for the effects of the relative motion of the space craft and the Earth.

We use light curves with a time resolution of $0.5\,\mathrm{ms}$. The time resolution is chosen small enough to preserve features
in the data on short time scales, while at the same time have a reasonable number of counts per time bin. 

The data is affected by both dead time and saturation. 
Dead time occurs because the instrument cannot record a second photon within $2.6\mu\mathrm{s}$ of arrival of a previous photon. 
The second photon is thus either not recorded at all, or, on occasion, recorded as a single photon with the combined energy. This effectively 
imposes a time scale onto the data, which means that the data then deviate away from the expected Poisson distribution. 
Dead time is harder to quantify and account for than saturation. It is possible to correct count rates based on simulations of the instrument (REF, Bhat et al, 2013),
but this will not correct the statistical distributions. We currently do not take dead time into account in our analysis. 

In addition to dead time, saturation may significantly alter the shape of the arriving bursts. Saturation occurs when the number of arriving photons per second
measured in a single GBM detector exceed the maximum data throughput rate of the science data bus. In this case, transmitted rates are capped at a 
count rate of $3.5 \times 10^{5} \, \mathrm{photons} \; \mathrm{s}^{-1}$, any photons exceeding that number are lost. For very bright bursts, this leads
to a flat-topped spikes truncated at the maximum count rate. Any bursts with count rates this high are excluded from the sample, as the model we
consider is not designed to represent these features. 

We currently use light curves with a time resolution of $0.5\,\mathrm{ms}$. The time resolution is chosen small enough to preserve features
in the data on short time scales, while at the same time have a reasonable number of counts per time bin. 
Burst durations are defined in $T_{90}$, the time in which $90\%$ of photons arrive at the detector, with $0.2T_{90}$ added on either side
to make sure the entire burst is included. 

%Data for the strongest-field magnetars SGR 1806-20 and SGR 1900+14 are recorded with the {\it Rossi} X-ray Timing Explorer (\RXTE), which someone
%else will write the details about (but data are still time-tagged photon events and suffer from saturation and dead time). 

%In what follows, the data for one burst are photon counts $\counts_n$ (integer) in $N$ bins $n$.
%The data from a few example bursts are shown in Figure \ref{fig:example_bursts}.


\section{Analysis Methods}

In order to successfully model the complex temporal variability in magnetar bursts, any modelling procedure must satisfy the following criteria: (1) it must be flexible enough to be applicable to a large number of bursts with distinctly different morphologies. We achieve this by decomposing magnetar burst light curves into one or more components with simple shapes, which, taken together, make up a burst. (2) The procedure must be largely automated, and be capable of sampling both the number of components as well as the model parameters for each component without human intervention. The latter is achieved by setting up a Bayesian model, where the number of components is a parameter to be sampled together with the corresponding parameters of the individual components. We use diffusive nested sampling as implemented in DNest \citep{brewer2011} to approximate the posterior distribution over all parameters. From samples of the posterior distribution,
we can then study the properties of individual burst components, as well as their properties for a given burst.

\subsection{A Bayesian model for magnetar burst light curves}
\label{sec:model}

For a light curve with $K$ Poisson-distributed counts $\bm{\counts} = \{\counts_k\}$, we define a model as a superposition of $N$ individual components:

%The model for the data in or near one burst is
\begin{eqnarray}
%p(\counts_n\given\pars) &=& \Poisson(\counts_n\given\mean_n)
%\\
\mean_n &=& \mean_{\bg} + \sum_{n=1}^N \mean_{nk}
\\
\mean_{nk} &\equiv& \int_{t_n-\Delta t/2}^{t_n+\Delta t/2} A_n\,\word(\frac{t'-t_n}{\tau})\,\dd t' \; ,
%\begin{equation}
%\word(\xi) &=& \left\{\begin{array}{ll}\exp(\xi) & \mbox{for $\xi<0$}\\ \exp(-\xi/s) & \mbox{for $\xi\geq 0$}\end{array}\right. \, ,
%\end{equation}
\end{eqnarray}

where $\mean_{nk}$ is the mean count rate of the $n^{\mathrm{th}}$ model component in time bin $k$, 
$\mean_{bg}$ is the background count rate of that bin,
and the mean count rate $\mean_{nk}$ in a bin $n$ is defined as the integral over a functional form defining the shape of
the model component $\word$ over the the width of that time bin. The component $\word$ is a generic shape,
and can be modified by an amplitude parameter $A_n$ and a parameter setting the width $\tau_n$, in addition to
parameters such as the time offset $t_n$ and intrinsic parameters further describing the component's shape.
We will define a component model $\word$ for magnetar bursts in Section \ref{sec:wordmodel} below, and
restrict ourselves here to a general description of the model.
 
%where $\Poisson(\counts\given\mean)$ is the Poisson probability of getting count $y$ given mean rate $\mean$,
%  $\pars$ is the vector or blob of all model parameters,
%  $\mean_{\bg}$ is the background (DC) level in the bin,
%  $t_n$ is the time of the center of the bin,
%  $\Delta$ is the full width of the bin,
%  $K$ is the number of words (spikes) $k$ making up the burst,
%  $\phi(\xi)$ is the dimensionless word function,
%  $A_n, t_n$ are the amplitudes and time offsets of the words,
%  and $s, \tau$ set the shape (asymmetry or skew) and rise time of the words.
  
We can compute the posterior probability over all model parameters in the following way:

\begin{equation}
p(N, \bm{\alpha},\{\bm{\theta}_n \} \given \bm{\counts}, H) = \frac{p(\bm{\counts} \given N, \bm{\alpha}, \{\bm{\theta}_n \}, H) p(N, \bm{\alpha}, \{\bm{\theta}_n \} \given H)}{p(\bm{\counts} | H)} \, .
\end{equation}

Here, $N$ is the number of model components, with the corresponding set of model components $\{\bm{\theta}_n\} = \{ \bm{\theta}_1, \bm{\theta}_2, ..., \bm{\theta}_N \}$. Each $\bm{\theta}_n$ may be a scalar, for models with a single parameter, or a vector.
The scalar or vector $\bm{\alpha}$ encodes the potential hyper parameters used to describe the prior distributions of parameters in $\{\bm{\theta}_n\}$. $H$ encodes the prior choices we have
made about the model that are not part of the inference problem at this stage: for example, the choice of shape for prior distributions and the model shape used to represent the light curve.

We use a Poisson likelihood to describe the data,

\begin{eqnarray}
\likelihood(N, \bm{\alpha}, \{\bm{\theta}_n \}) & = & p(\bm{\counts} \given N, \bm{\alpha}, \{\bm{\theta}_n \}, H) \\ \nonumber
 &= & \prod\limits_{k=0}^{K}{ \frac{e^{-\lambda} \lambda^{y_k} }{y_k! }} \; ,
\end{eqnarray}

which only depends on the parameters of the individual model components, $\{\bm{\theta}_n\}$. In general, the interim priors for the model 
parameters $\{\bm{\theta}_n\}$ will depend on the priors for the hyper parameters defining their prior distributions, $\bm{\alpha}$, such that the
overall prior for this model is

\begin{equation}
p(N, \bm{\alpha}, \{\bm{\theta}_n \} \given H) = p(N)p(\alpha\given N)p(\{\bm{\theta}_n\}\given \alpha, N) \; .
\end{equation}

Under the assumption that $\bm{\alpha}$ and $N$ are independent, and that the interim priors of the individual model components are
independent and identically distributed, we can re-write this equation as:

\begin{equation}
p(N, \bm{\alpha}, \{\bm{\theta}_n \} \given H) = p(N)p(\alpha) \prod\limits_{n=0}^{N}  p(\{\bm{\theta}_n\}\given \alpha) \; .
\end{equation}

Thus, the posterior probability density becomes

\begin{eqnarray}
p(N, \bm{\alpha}, &&\{\bm{\theta}_n \}  \given \bm{\counts}, H) = \\ 
&& \frac{\prod\limits_{k=0}^{K}{ \frac{e^{-\lambda} \lambda^{y_k} }{y_k! }} p(N)p(\alpha) \prod\limits_{n=0}^{N}  p(\{\bm{\theta}_n\}\given \alpha)}{p(\bm{\counts} | H)}  \nonumber
\end{eqnarray}

where the marginal likelihood, $p(\bm{\counts} | H)$ is defined as a normalisation constant in the usual way: 

\begin{eqnarray}
p(\bm{\counts} | H) & = & \int_{-\infty}^{\infty}{\likelihood(N, \bm{\alpha}, \{\bm{\theta}_n \})} \times \\ \nonumber
&& p(N)p(\alpha) \prod\limits_{n=0}^{N}  p(\{\bm{\theta}_n\}\given \alpha) d\bm{\alpha} dN d\bm{\theta}_1 ... d\bm{\theta}_N \; .
\end{eqnarray}

The marginal likelihood involves integration in a high-dimensional parameter space, which is analytically intractable for all but the
simplest cases. Here, we use Diffusive Nested Sampling \citep{brewer2011} to efficiently traverse parameter space and approximate the marginal 
likelihood as well as sample from the posterior probability density. 
Nested sampling \citep{skilling2006} samples parameter space by uniformly sampling $M$ points (particles) from the space allowed by the prior. 
One then computes the likelihood values associated with each particle, and the particle with the lowest likelihood is discarded. A new point
can be generated, for example, via Markov Chain Monte Carlo (MCMC), subject to the hard constraint that the
likelihood of the new point must be larger than the likelihood of the discarded one. In this way, the population iterates progressively towards
regions of higher likelihood. At the same time, one may assign a value $X  \in [0, 1]$ to the discarded particle. This effectively provides a 
mapping from parameter space into $[0,1]$, such that the evaluation of the marginal likelihood simplifies to a one-dimensional 
integral tractable with numerical integration methods.

The MCMC step is the key challenge in any Nested Sampling algorithm, and classical MCMC often fails for probability distributions that
are highly multi-modal or contain phase transitions. It is here that Diffusive Nested Sampling provides a reliable alternative \citep[for details, see][]{brewer2011}. 
Instead of recording the end point of an MCMC chain as the new particle, the likelihoods of parameter sets visited during the MCMC exploration step
 are recorded. One can then compute the $1-e^{-1}$ quantile, and record this value as the new likelihood contour, such that the prior space
 under consideration contracts by a factor of $\sim e$. Instead of enforcing a hard likelihood constraint, one now samples a mixture of the 
 two regions, and the particle may escape into the lower-likelihood region, where it can explore more freely. Once enough samples are accumulated,
 one again computes the $1-e^{-1}$ quantile, and constructs a new likelihood contour. This process is repeated, and as in the classical Nested Sampling
 approach, the population contracts towards regions of high likelihood, but it will do so even for complex probability distributions subject to multi-modality
 or phase transitions.

Diffusive nested sampling as implemented in {\it DNest3}\footnote{the code is available under the GNU public license at \hyperref[]{https://github.com/eggplantbren/DNest3}} can return both an estimate for the marginal likelihood,
suitable for model selection purposes, as well as a samples drawn from the posterior probability density function (pdf) for a given burst. The latter is done by letting a particle explore
the final mixture of levels freely via MCMC. 
We can make inferences about individual parameters in the model from these samples in the usual way by marginalising over nuisance parameters (e.g. 
hyper parameters $\bm{\alpha}$). Additionally, for many bursts, we can make inferences over the parameters from many components. In the naive way,
this can be done by picking a parameter set from the MCMC run for each burst, and using this ensemble of individual samples from many bursts 
to compute the quantity of interest (say, a correlation between two model parameters). This procedure can be repeated, such that we build up a sample
for the quantity in question. Note, however, that by posing that individual bursts have no knowledge of each other - we fit the model for each burst separately - 
this imposes a strong prior on this quantity, suggesting that there is, in fact, no correlation present. 
For the type of exploratory data analysis considered here, this is sufficient. A formally correct implementation would have to include a Bayesian hierarchical model,
where many bursts are considered in the same model, with appropriate priors on any quantities to be measured across samples of many bursts. This can, in turn,
be extended again to make inferences across, for example, different magnetars. The latter two steps are beyond the scope of this work, but will be explored in the future.


\subsection{Model shapes}
\label{sec:wordmodel}

The model defined in Section \ref{sec:model} is fairly general: it is valid for any Poisson-distributed light curve thought to be composed several individual components of
the same shape, but with potentially different individual parameters (such as amplitudes and widths). 
We have made only three assumptions so far: the likelihood follows a Poisson distribution, the priors for the number of components $N$ and the hyper parameters $\bm{\alpha}$ are independent,
and the priors on the parameters of the model for the individual components are independent of each other and identically distributed. 
Here, we now refine this model for magnetar bursts specifically. However, changing the model shape for use with different source classes (e.g. GRBs) is straightforward.
\begin{figure}[h]
\begin{center}
\includegraphics[width=9cm]{word.pdf}
\caption{An example of the component model used for magnetar bursts: a spike is defined by an exponential rise with characteristic
time scale $\tau$ and an exponential fall with a fall time scale $s\tau$, where $s$ is a skewness parameter that describes how the fall
time is stretched ($s > 1$) of contracted ($s < 1$) compared to the rise time. Individual spikes are also characterised by a time offset
$t$ describing the location of the peak count rate in a time series, and an amplitude $A$ describing the height of a peak.}
\label{fig:word_example}
\end{center}
\end{figure}
Magnetar bursts are short events composed of one or more sharp, spike-like features (see Figure \ref{fig:example_bursts} for various examples). We model these features
with a sharp spike consisting of an exponential rise and an exponential decay of the type:


%The model for the data in or near one burst is
%\begin{eqnarray}
%p(\counts_n\given\pars) &=& \Poisson(\counts_n\given\mean_n)
%\\
%\mean_n &=& \mean_{\bg} + \sum_{k=1}^K \mean_{nk}
%\\
%\mean_{nk} &\equiv& \int_{t_n-\Delta/2}^{t_n+\Delta/2} A_n\,\word(\frac{t'-t_n}{\tau})\,\dd t'
%\\
\begin{equation}
\word(t) = A \left\{\begin{array}{ll}\exp(t-t_{\mathrm{offset}}/\tau) & \mbox{for $t < t_{\mathrm{offset}}$}\\ 
\exp(-(t-t_{\mathrm{offset}})/(\tau s)) & \mbox{for $t \geq t_\mathrm{offset}$}\end{array}\right. \, ,
\end{equation}
%\end{eqnarray}

where $\word(ti)$ is the component function depending on time parameter $t$ and a skewness
parameter $s$. By giving each component $n$ in the model a time offset $t_{\mathrm{offset},n}$ (equivalent to the time of the peak), 
an amplitude $A_n$ and an exponential rise timescale $\tau_n$ in addition to the skewness parameter $s_n$, 
these spikes can become a representation of the spikes observed in magnetar burst light curves (see Figure \ref{fig:word_example}
for an example of the model).



For each component $n$, we have a set of free parameters $\{t_n, A_n, \tau_n, s_n \},$, for each model, i.e. each
linear combination of components, we have a set of free parameters $\{\mean_{bg}, \{t_n, A_n, \tau_n, s_n\} \}$.
The priors for these components are independent and identically distributed, such that priors are the same for
a given type of parameter between components. 
Because bursts are defined in such a way that the observed count rate must drop back to the background before the
start and the end of each burst, we can simply define the prior on the time offset such that $t_n$ must lie between
the start and end times of the burst light curve, $t_{\mathrm{start}} < t_n < t_\mathrm{end}$. 
For the amplitude $A_n$ and the rise timescale $\tau_n$, we choose exponential priors:

\begin{eqnarray}
p(A \given \mu_A) &=& e^{(A/\mu_A)} \\
p(\tau \given \mu_{\tau} & = & e^{(\frac{\tau - \tau_{\mathrm{min}}}{\mu_{\mathrm{\tau}}})} \; ,
\end{eqnarray}

where $\mu_A$ and $\mu_{\mathrm{\tau}}$ are hyper parameters describing the width of the exponential distribution.
We set a minimum possible rise time scale $\tau_{\mathrm{min}}$ to be a fraction of the light curve's time resolution,
$\tau_{\mathrm{min}} = \Delta t/10$: the time resolution effectively limits the information on the rise time we can extract.

The prior on the skewness parameter $s_n$ is a uniform distribution with a mean of $a$ and a half-width of $b$, such
that the log-skewness must lie in the range $(a-b) < \log{(s)} < (a+b)$. A definition in terms of mean and half-width ensures
that positive and negative skews are equally likely a priori.

\begin{figure*}[h]
\begin{center}
\includegraphics[width=\textwidth]{example_dnest_result.pdf}
\caption{An example burst from the magnetar SGR J1550-5418, in an observation taken on 2009 January 22 (ObsID 090122173). In the left
panel, the light curve at high time resolution, $\Delta t = 5 \times 10^{-4}$, and model light curves for $10$ random draws from the posterior pdf. 
On the right, the marginalised posterior pdf over the possible number of components in the model. The posterior for the number of components is 
quite well constrained between $9$ and $17$ components.}
\label{fig:dnest_example}
\end{center}
\end{figure*}

The prior on the background parameter, finally, depends on the data itself. It is log-uniform, with boundaries between 
$\log{(10^{-3}\mu_\counts)} < \log{(\counts_{\mathrm{bg}})} < \log{(10^{3} \mu_\counts)}$, where $\mu_\counts$ is the average count
rate in a burst. Note that this is, in principle, not a valid prior: the prior for the parameters should not depend on the data
itself. In practice, it is convenient as long as one does not need to compute the marginal likelihood. In a more in-depth analysis
involving the marginal likelihood where, for example, one might test whether any results change with a change of prior, or where different component models
might be compared, one would need to discard this prior for one that is formally valid.

The priors on the hyper parameters $\mu_{A}$ and $\mu_{\mathrm{\tau}}$ are log-uniform. The prior range for
$\mu_{\tau}$ depends on the length of the data set, such that $\log{(10^{-3}t_{\mathrm{burst}})} < \log{(\mu_{\tau})} < \log{(10^{3}t_\mathrm{burst})}$.
Like the prior on the background count rate $\counts_{\mathrm{bg}}$, the hyper prior on the width of the amplitude prior distribution
depends on the count rate in a burst, $\mu_\counts$: $\log{(10^{-3}\mu_\counts)} < \log{(\mu_{A})} < \log{(10^{3} \mu_\counts)}$. The same reasoning applies as
for the background prior, such that for a more complicated analysis involving the marginal likelihood it needs to be replaced
by a prior that is formally valid.

We show an example of a burst light curve, together with random draws from the posterior distribution, in Figure \ref{fig:dnest_example}.



%where $\Poisson(\counts\given\mean)$ is the Poisson probability of getting count $y$ given mean rate $\mean$,
%  $\pars$ is the vector or blob of all model parameters,
%  $\mean_{\bg}$ is the background (DC) level in the bin,
%  $t_n$ is the time of the center of the bin,
%  $\Delta$ is the full width of the bin,
%  $K$ is the number of words (spikes) $k$ making up the burst,
%  $\phi(\xi)$ is the dimensionless word function,
%  $A_n, t_n$ are the amplitudes and time offsets of the words,
%  and $s, \tau$ set the shape (asymmetry or skew) and rise time of the words.
  
%Various parameters can be left free completely, or can be tied together between words:
 
%\begin{equation}
%\theta \equiv [,\{t_n, \tau_n, A_n, s_n \}_{k=1}^K, \mean_{\bg} ]
%\end{equation}

%for complete freedom of word parameters between words,

%\begin{equation}
%\theta \equiv [,\{t_n, A_n, s_n \}_{k=1}^K,  \tau_n, \mean_{\bg} ]
%\end{equation}

%for tying the rise time together within a burst, and

%\begin{equation}
%\theta \equiv [,\{t_n, A_n\}_{k=1}^K, \tau_n, s_n , \mean_{\bg} ]\end{equation}

%for a model where both the rise time and the skewness are tied together for all words within a burst.

%\begin{figure*}[h]
%\begin{center}
%\includegraphics[width=18cm]{example_words.png}
%\caption{}
%\label{fig:example_words}
%\end{center}
%\end{figure*}

  
%Figure \ref{fig:example_words} shows the shape of a word
%  and an example of a burst generated by our model.

%The model for a large set of bursts is whatever.
%[We should probably, in the above, index bursts $m$,
%  and maybe even magnetar $j$.]

%\emph{Notes to selves:}
%At one point we figured we could \emph{fix} shape parameter $s$
%  to be the same for all the words in all the bursts from one magnetar,
%  but permit the duration $\tau$ to vary.
%However, as we worked in our coding session we moved more in the direction
%  of thinking in terms of ``rise time'' $\tau$ and decay time $s\,\tau$.
%It might be better to parameterize that way.
%One possible point of discovery would be that the rise times
%  are fundamental to each magnetar,
%  but decay times vary.
%(For instance.)

%The prior pdfs are
%\begin{eqnarray}
%p(\ln\lambda_{\bg}) &=& \Uniform(\ln\lambda_{\bg}\given a_1, a_2)
%\\
%p(\ln s) &=& \Uniform(\ln s\given a_3, a_4)
%\\
%p(\ln\tau) &=& \Uniform(\ln\tau\given a_5, a_6)
%\\
%p(\ln A_n) &=& \Uniform(\ln A_n\given a_7, a_8)
%\\
%p(t_n) &=& \Uniform(t_n\given a_9, a_{10})
%\quad,
%\end{eqnarray}
%where $\Uniform(x\given a, b)$ is the uniform distribution for $x$ in the range $a<x<b$.

%We use \project{emcee} (CITE) to perform MCMC sampling.
%[We initialize the MCMC walkers how?]

%Finally, there is the issue of how to set the number $K$ of words in each burst.
%We take a ``cross-validation'' approach [take that, BJB],
%  in which we ask what value of $K$ does the best job at making posterior predictions
%  about left-out data.
%[We construct this validation test how?]
%Figure [what figure?] shows an example burst,
%  fit with $K=1$, $2$, $3$, $4$, and $5$ words.
%It also shows the result of the posterior-prediction validation
%  that selects $K=4$ as the best choice for this burst.

\section{Results}



\section{Discussion}


\paragraph{acknowledgements}
We thank the organizers of MaxEnt2013.


\bibliography{td}
\bibliographystyle{apj}

\end{document}
